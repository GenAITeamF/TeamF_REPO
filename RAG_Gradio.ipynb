{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "664d7abc1cca415e9589016331fa5541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fec0da8aed894284bba8c0552ab89ceb",
              "IPY_MODEL_bd3e5866a2c9499e8adf4f8f8c975072",
              "IPY_MODEL_326ab3e765a143cc957e9ba8d301289d"
            ],
            "layout": "IPY_MODEL_8180bd1c04e44bcf8c20e7b5f5934200"
          }
        },
        "fec0da8aed894284bba8c0552ab89ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72018d5a3312488684b2755712b9fa4b",
            "placeholder": "​",
            "style": "IPY_MODEL_ed7332ea28dc4f6cb9f1d5177c98f2b6",
            "value": "Batches: 100%"
          }
        },
        "bd3e5866a2c9499e8adf4f8f8c975072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ab2e55a0c34dc186f5ea0021acd4c6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c07c20fc84474a41939f3b8a6d11ddfa",
            "value": 3
          }
        },
        "326ab3e765a143cc957e9ba8d301289d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13ddd001e7c49f983fd97cea21d38d8",
            "placeholder": "​",
            "style": "IPY_MODEL_5a569fdc62f7467e9fb57c02a9578b35",
            "value": " 3/3 [00:00&lt;00:00,  2.69it/s]"
          }
        },
        "8180bd1c04e44bcf8c20e7b5f5934200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72018d5a3312488684b2755712b9fa4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7332ea28dc4f6cb9f1d5177c98f2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ab2e55a0c34dc186f5ea0021acd4c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07c20fc84474a41939f3b8a6d11ddfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a13ddd001e7c49f983fd97cea21d38d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a569fdc62f7467e9fb57c02a9578b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "33syeG_xXIc6",
        "outputId": "9bf50706-c89d-4abb-ba4a-c90454b1b663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: semchunk in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.1)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: mpire[dill] in /usr/local/lib/python3.10/dist-packages (from semchunk) (2.10.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk) (2.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk) (0.70.17)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: dill>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from multiprocess->mpire[dill]->semchunk) (0.3.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio transformers sentence_transformers openai PyPDF2 pdfplumber bs4 semchunk faiss-gpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "from fastapi import FastAPI\n",
        "from datetime import datetime\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader  # PDF 분석을 위한 라이브러리\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from ProcFile import process_file\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"EMPTY\",\n",
        "    base_url=\"https://bf60-35-198-247-133.ngrok-free.app/v1\"  # ngrok URL로 교체\n",
        ")\n",
        "\n",
        "###\n",
        "# 모델 로드\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# 글로벌 변수\n",
        "res = faiss.StandardGpuResources()  # GPU 리소스 생성\n",
        "text_faiss_index = None\n",
        "image_faiss_index = None\n",
        "chunk_data = []\n",
        "chunk_metadata = []\n",
        "image_embeddings = []\n",
        "image_metadata = []\n",
        "GPU_FLAG =False\n",
        "\n",
        "\n",
        "# 장치 선택 처리\n",
        "if torch.cuda.is_available():\n",
        "  cuda_version = torch.version.cuda\n",
        "  gpu_name = torch.cuda.get_device_name(0)\n",
        "  GPU_FLAG = True\n",
        "###\n",
        "\n",
        "\n",
        "# Initialize global variables\n",
        "chat_rooms = [{\"id\": \"Chat1\", \"timestamp\": datetime.now()}]\n",
        "chat_histories = {\"Chat1\": []}\n",
        "ai_history = {\"Chat1\": []}\n",
        "current_chat = \"Chat1\"\n",
        "\n",
        "def create_new_chat():\n",
        "    global chat_rooms, chat_histories, current_chat, ai_history\n",
        "    new_chat_id = f\"Chat{len(chat_rooms) + 1}\"\n",
        "    chat_rooms.append({\"id\": new_chat_id, \"timestamp\": datetime.now()})\n",
        "    chat_histories[new_chat_id] = []\n",
        "    ai_history[new_chat_id] = []\n",
        "    current_chat = new_chat_id\n",
        "    return (\n",
        "        gr.update(choices=[room[\"id\"] for room in chat_rooms], value=new_chat_id),\n",
        "        [],\n",
        "    )\n",
        "#이 부분에서 faiss 저장 & search까지\n",
        "def extract_pdf_text(file_path):\n",
        "    \"\"\"PDF 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF file: {str(e)}\"\n",
        "\n",
        "def search_file(query):\n",
        "    global text_faiss_index, chunk_metadata\n",
        "\n",
        "    if text_faiss_index is None:\n",
        "        return \"No indexed data. Please upload and process a file first.\"\n",
        "\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    distances, indices = text_faiss_index.search(query_embedding, 5)\n",
        "\n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if distance > 100:\n",
        "            continue\n",
        "        if idx < len(chunk_metadata):\n",
        "            meta = chunk_metadata[idx]\n",
        "            results.append(\n",
        "                f\"Result {i + 1}: File: {meta['file']} | Type: {meta['type']} | \"\n",
        "                f\"{'Page' if meta['type'] == 'PDF' else 'Row'}: {meta.get('page', meta.get('row', 'N/A'))} | \"\n",
        "                f\"Distance: {distance:.4f}\\n\"\n",
        "                f\"Content: {meta['chunk_text'][:200]}...\"\n",
        "            )\n",
        "        top_chunks = [item.split('Content: ')[1] for item in results[:3]]\n",
        "        print(\"\\n\".join(results) if results else \"No relevant results found within the distance threshold.\")\n",
        "    return top_chunks\n",
        "\n",
        "\n",
        "def add_message(history, message):\n",
        "    global current_chat, chat_histories, ai_history, text_faiss_index\n",
        "\n",
        "    if current_chat is None:\n",
        "        _, history = create_new_chat()\n",
        "    else:\n",
        "        history = chat_histories[current_chat]\n",
        "\n",
        "    ai_content = []\n",
        "\n",
        "    if message[\"files\"]:\n",
        "        for file_path in message[\"files\"]:\n",
        "            print(file_path)\n",
        "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                #llava는 사진업로드 1장만 가능; AI 쿼리용 history에서 이미지 삭제\n",
        "                # for message in ai_history[current_chat]:\n",
        "                #     if 'content' in message and isinstance(message['content'], list):\n",
        "                #         message['content'] = [item for item in message['content'] if item.get('type') != 'image_url']\n",
        "\n",
        "                try:\n",
        "                    history.append({\"role\": \"user\", \"content\": {\"path\": {file_path}}})\n",
        "\n",
        "                    # 이미지 Base64 데이터를 AI쿼리용 history의 content에 기록\n",
        "                    with open(file_path, \"rb\") as image_file:\n",
        "                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "                        image_base64 = f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                        ai_content.append({\"type\" : \"image_url\", \"image_url\": {\"url\" : image_base64}})\n",
        "\n",
        "                except Exception as e:\n",
        "                    history.append({\"role\": \"assistant\", \"content\": f\"Error processing file {file_path}: {str(e)}\"})\n",
        "\n",
        "            elif file_path.lower().endswith('.pdf'):\n",
        "                # PDF 파일 처리\n",
        "                pdf_text = extract_pdf_text(file_path)      #이 부분!!\n",
        "                if \"Error\" in pdf_text:\n",
        "                    history.append({\"role\": \"assistant\", \"content\": pdf_text})\n",
        "                else:\n",
        "                    #파일 프로세스\n",
        "                    with open(file_path, 'rb') as file:\n",
        "                        file_name, text_chunks, metadata_chunks = process_file(file)\n",
        "                    # file_name, text_chunks, metadata_chunks = process_file(file_path)\n",
        "                    chunk_data.extend(text_chunks)\n",
        "                    chunk_metadata.extend(metadata_chunks)\n",
        "\n",
        "                    embeddings = embedding_model.encode(text_chunks, batch_size=32, show_progress_bar=True)\n",
        "                    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "                    d = embeddings.shape[1]\n",
        "\n",
        "                    if text_faiss_index is None:\n",
        "                        if(GPU_FLAG):\n",
        "                            # GPU용 인덱스 생성\n",
        "                            flat_config = faiss.GpuIndexFlatConfig()\n",
        "                            flat_config.device = 0  # 사용할 GPU 장치 ID 설정 (보통 0번부터 시작)\n",
        "                            text_faiss_index = faiss.GpuIndexFlatL2(res, d, flat_config)\n",
        "                        else:\n",
        "\n",
        "                            text_faiss_index = faiss.IndexFlatL2(d)\n",
        "                    text_faiss_index.add(embeddings)\n",
        "\n",
        "                    print(f\"{file_name} processed successfully. {len(text_chunks)} chunks indexed.\")\n",
        "\n",
        "                    ##쿼리로부터 서치, 결과 + 쿼리 hisotry에 추가\n",
        "                    file_search = []\n",
        "                    if (message[\"text\"] != None):\n",
        "                        topk = search_file(message[\"text\"])\n",
        "                    for index, result in enumerate(topk):\n",
        "                        #history.append({\"role\": \"user\", \"content\": result})\n",
        "                        file_search.append(result)\n",
        "                    ai_content.append({\"type\" : \"text\", \"text\" : f\"Based on the following three sentences: 1. {file_search[0]}, 2. {file_search[1]}, 3. {file_search[2]}\"})\n",
        "                    print(\"search done!\")\n",
        "                    #history.append({\"role\": \"user\", \"content\": f\"PDF uploaded: {file_path}\\nExtracted text: {pdf_text[:500]}...\"}) #이 부분!!!\n",
        "            else:\n",
        "                # 이미지 파일이 아닌 경우\n",
        "                history.append({\"role\": \"user\", \"content\": f\"File {file_path} is not supported.\"})\n",
        "\n",
        "    if (message[\"text\"]) == None:\n",
        "        history.append({\"role\": \"user\", \"content\": \"Please input any message\"})\n",
        "        return\n",
        "    elif (message[\"text\"]):\n",
        "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
        "        ai_content.append({\"type\" : \"text\", \"text\" : message[\"text\"]})\n",
        "\n",
        "    print(ai_content)\n",
        "    # 업데이트된 히스토리를 저장\n",
        "    chat_histories[current_chat] = history\n",
        "    ai_history[current_chat] = [{\"role\": \"user\", \"content\" : ai_content}]\n",
        "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
        "\n",
        "def bot(history: list):\n",
        "    global current_chat, chat_histories, ai_history\n",
        "\n",
        "    try:\n",
        "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "        # for msg in history:\n",
        "        #     if msg.get(\"image\"):\n",
        "        #         messages.append({\"role\": \"user\", \"content\": f\"Image data: {msg['image']}\"})\n",
        "        #     else:\n",
        "        #         messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "            messages=ai_history[current_chat],\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                history[-1][\"content\"] += chunk.choices[0].delta.content\n",
        "                time.sleep(0.05)\n",
        "                yield history\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"})\n",
        "        yield history\n",
        "\n",
        "    chat_histories[current_chat] = history\n",
        "\n",
        "def switch_chat(chat_id):\n",
        "    global current_chat, chat_histories\n",
        "    current_chat = chat_id\n",
        "    return chat_histories.get(chat_id, [])\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            new_chat_button = gr.Button(\"New Chat\")\n",
        "            chat_list = gr.Dropdown(\n",
        "                label=\"Chat Rooms\",\n",
        "                choices=[room[\"id\"] for room in chat_rooms],\n",
        "                value=\"Chat1\",\n",
        "            )\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(type=\"messages\")\n",
        "            chat_input = gr.MultimodalTextbox(\n",
        "                interactive=True,\n",
        "                file_count=\"multiple\",\n",
        "                placeholder=\"Enter message or upload file...\",\n",
        "                show_label=False,\n",
        "            )\n",
        "\n",
        "    chat_msg = chat_input.submit(\n",
        "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
        "    )\n",
        "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
        "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
        "\n",
        "    new_chat_button.click(create_new_chat, [], [chat_list, chatbot])\n",
        "    chat_list.change(switch_chat, inputs=[chat_list], outputs=[chatbot])\n",
        "\n",
        "#gr.mount_gradio_app(app, demo, path=\"/gradio\")\n",
        "demo.launch(debug=True).share=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "664d7abc1cca415e9589016331fa5541",
            "fec0da8aed894284bba8c0552ab89ceb",
            "bd3e5866a2c9499e8adf4f8f8c975072",
            "326ab3e765a143cc957e9ba8d301289d",
            "8180bd1c04e44bcf8c20e7b5f5934200",
            "72018d5a3312488684b2755712b9fa4b",
            "ed7332ea28dc4f6cb9f1d5177c98f2b6",
            "d2ab2e55a0c34dc186f5ea0021acd4c6",
            "c07c20fc84474a41939f3b8a6d11ddfa",
            "a13ddd001e7c49f983fd97cea21d38d8",
            "5a569fdc62f7467e9fb57c02a9578b35"
          ]
        },
        "id": "4qldkGWiXOf7",
        "outputId": "88d23e6c-956d-41ba-b043-64efd4765ece"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f0b8994821588c8a18.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0b8994821588c8a18.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/gradio/587909888c9923e3fa0dc8f6818d9ca5566f2bf63568dd6f94b54b6dd2d9704a/Lecture-01-LLM-Input-Output.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664d7abc1cca415e9589016331fa5541"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lecture-01-LLM-Input-Output.pdf processed successfully. 68 chunks indexed.\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab size로 부터 encode된 token id를 vector로 1대1 mapping vector set는 vocab size 만큼의 vector table로 구성 embedded vector는 learnable paramet...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab size로 부터 encode된 token id를 vector로 1대1 mapping vector set는 vocab size 만큼의 vector table로 구성 embedded vector는 learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: 좌표시스템 대신에, 임의의 vector space를 W matrix로 표현하면, vector x를 W에 projection하는 식은, 𝑊𝑥𝑇 가 된다. LLM에서는 token embedding에 사용된 nn.Embedding weights를 재사용 weight sharing Weights matrix를 W라 하고, output sequence 를 matri...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab size로 부터 encode된 token id를 vector로 1대1 mapping vector set는 vocab size 만큼의 vector table로 구성 embedded vector는 learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: 좌표시스템 대신에, 임의의 vector space를 W matrix로 표현하면, vector x를 W에 projection하는 식은, 𝑊𝑥𝑇 가 된다. LLM에서는 token embedding에 사용된 nn.Embedding weights를 재사용 weight sharing Weights matrix를 W라 하고, output sequence 를 matri...\n",
            "Result 4: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 16 | Distance: 1.4060\n",
            "Content: Position Embedding in LLM Why do we need positional embedding? Transformer Network에는 sequence order개념을 처리할 능력이 없다. ① matrixmultiplication ② layer normalization ③ MLP multilayer perceptron ④ attention ...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab size로 부터 encode된 token id를 vector로 1대1 mapping vector set는 vocab size 만큼의 vector table로 구성 embedded vector는 learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: 좌표시스템 대신에, 임의의 vector space를 W matrix로 표현하면, vector x를 W에 projection하는 식은, 𝑊𝑥𝑇 가 된다. LLM에서는 token embedding에 사용된 nn.Embedding weights를 재사용 weight sharing Weights matrix를 W라 하고, output sequence 를 matri...\n",
            "Result 4: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 16 | Distance: 1.4060\n",
            "Content: Position Embedding in LLM Why do we need positional embedding? Transformer Network에는 sequence order개념을 처리할 능력이 없다. ① matrixmultiplication ② layer normalization ③ MLP multilayer perceptron ④ attention ...\n",
            "Result 5: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 23 | Distance: 1.4738\n",
            "Content: Input Output Training Example for LM This is a sample chunk of words. This is a sample chunk of words. The transformer model should The transformer model should predict next predict next word embeddin...\n",
            "search done!\n",
            "[{'type': 'text', 'text': 'Based on the following three sentences: 1. LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme..., 2. Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab size로 부터 encode된 token id를 vector로 1대1 mapping vector set는 vocab size 만큼의 vector table로 구성 embedded vector는 learnable paramet..., 3. 좌표시스템 대신에, 임의의 vector space를 W matrix로 표현하면, vector x를 W에 projection하는 식은, 𝑊𝑥𝑇 가 된다. LLM에서는 token embedding에 사용된 nn.Embedding weights를 재사용 weight sharing Weights matrix를 W라 하고, output sequence 를 matri...'}, {'type': 'text', 'text': 'tell me about llm'}]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f0b8994821588c8a18.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scVGd5Za2Iwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}