{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "664d7abc1cca415e9589016331fa5541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fec0da8aed894284bba8c0552ab89ceb",
              "IPY_MODEL_bd3e5866a2c9499e8adf4f8f8c975072",
              "IPY_MODEL_326ab3e765a143cc957e9ba8d301289d"
            ],
            "layout": "IPY_MODEL_8180bd1c04e44bcf8c20e7b5f5934200"
          }
        },
        "fec0da8aed894284bba8c0552ab89ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72018d5a3312488684b2755712b9fa4b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ed7332ea28dc4f6cb9f1d5177c98f2b6",
            "value": "Batches:â€‡100%"
          }
        },
        "bd3e5866a2c9499e8adf4f8f8c975072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ab2e55a0c34dc186f5ea0021acd4c6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c07c20fc84474a41939f3b8a6d11ddfa",
            "value": 3
          }
        },
        "326ab3e765a143cc957e9ba8d301289d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13ddd001e7c49f983fd97cea21d38d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5a569fdc62f7467e9fb57c02a9578b35",
            "value": "â€‡3/3â€‡[00:00&lt;00:00,â€‡â€‡2.69it/s]"
          }
        },
        "8180bd1c04e44bcf8c20e7b5f5934200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72018d5a3312488684b2755712b9fa4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7332ea28dc4f6cb9f1d5177c98f2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ab2e55a0c34dc186f5ea0021acd4c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07c20fc84474a41939f3b8a6d11ddfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a13ddd001e7c49f983fd97cea21d38d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a569fdc62f7467e9fb57c02a9578b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "33syeG_xXIc6",
        "outputId": "9bf50706-c89d-4abb-ba4a-c90454b1b663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: semchunk in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.1)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: mpire[dill] in /usr/local/lib/python3.10/dist-packages (from semchunk) (2.10.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk) (2.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk) (0.70.17)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: dill>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from multiprocess->mpire[dill]->semchunk) (0.3.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio transformers sentence_transformers openai PyPDF2 pdfplumber bs4 semchunk faiss-gpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "from fastapi import FastAPI\n",
        "from datetime import datetime\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader  # PDF ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from ProcFile import process_file\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"EMPTY\",\n",
        "    base_url=\"https://bf60-35-198-247-133.ngrok-free.app/v1\"  # ngrok URLë¡œ êµì²´\n",
        ")\n",
        "\n",
        "###\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# ê¸€ë¡œë²Œ ë³€ìˆ˜\n",
        "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ ìƒì„±\n",
        "text_faiss_index = None\n",
        "image_faiss_index = None\n",
        "chunk_data = []\n",
        "chunk_metadata = []\n",
        "image_embeddings = []\n",
        "image_metadata = []\n",
        "GPU_FLAG =False\n",
        "\n",
        "\n",
        "# ìž¥ì¹˜ ì„ íƒ ì²˜ë¦¬\n",
        "if torch.cuda.is_available():\n",
        "  cuda_version = torch.version.cuda\n",
        "  gpu_name = torch.cuda.get_device_name(0)\n",
        "  GPU_FLAG = True\n",
        "###\n",
        "\n",
        "\n",
        "# Initialize global variables\n",
        "chat_rooms = [{\"id\": \"Chat1\", \"timestamp\": datetime.now()}]\n",
        "chat_histories = {\"Chat1\": []}\n",
        "ai_history = {\"Chat1\": []}\n",
        "current_chat = \"Chat1\"\n",
        "\n",
        "def create_new_chat():\n",
        "    global chat_rooms, chat_histories, current_chat, ai_history\n",
        "    new_chat_id = f\"Chat{len(chat_rooms) + 1}\"\n",
        "    chat_rooms.append({\"id\": new_chat_id, \"timestamp\": datetime.now()})\n",
        "    chat_histories[new_chat_id] = []\n",
        "    ai_history[new_chat_id] = []\n",
        "    current_chat = new_chat_id\n",
        "    return (\n",
        "        gr.update(choices=[room[\"id\"] for room in chat_rooms], value=new_chat_id),\n",
        "        [],\n",
        "    )\n",
        "#ì´ ë¶€ë¶„ì—ì„œ faiss ì €ìž¥ & searchê¹Œì§€\n",
        "def extract_pdf_text(file_path):\n",
        "    \"\"\"PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF file: {str(e)}\"\n",
        "\n",
        "def search_file(query):\n",
        "    global text_faiss_index, chunk_metadata\n",
        "\n",
        "    if text_faiss_index is None:\n",
        "        return \"No indexed data. Please upload and process a file first.\"\n",
        "\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "\n",
        "    distances, indices = text_faiss_index.search(query_embedding, 5)\n",
        "\n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if distance > 100:\n",
        "            continue\n",
        "        if idx < len(chunk_metadata):\n",
        "            meta = chunk_metadata[idx]\n",
        "            results.append(\n",
        "                f\"Result {i + 1}: File: {meta['file']} | Type: {meta['type']} | \"\n",
        "                f\"{'Page' if meta['type'] == 'PDF' else 'Row'}: {meta.get('page', meta.get('row', 'N/A'))} | \"\n",
        "                f\"Distance: {distance:.4f}\\n\"\n",
        "                f\"Content: {meta['chunk_text'][:200]}...\"\n",
        "            )\n",
        "        top_chunks = [item.split('Content: ')[1] for item in results[:3]]\n",
        "        print(\"\\n\".join(results) if results else \"No relevant results found within the distance threshold.\")\n",
        "    return top_chunks\n",
        "\n",
        "\n",
        "def add_message(history, message):\n",
        "    global current_chat, chat_histories, ai_history, text_faiss_index\n",
        "\n",
        "    if current_chat is None:\n",
        "        _, history = create_new_chat()\n",
        "    else:\n",
        "        history = chat_histories[current_chat]\n",
        "\n",
        "    ai_content = []\n",
        "\n",
        "    if message[\"files\"]:\n",
        "        for file_path in message[\"files\"]:\n",
        "            print(file_path)\n",
        "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                #llavaëŠ” ì‚¬ì§„ì—…ë¡œë“œ 1ìž¥ë§Œ ê°€ëŠ¥; AI ì¿¼ë¦¬ìš© historyì—ì„œ ì´ë¯¸ì§€ ì‚­ì œ\n",
        "                # for message in ai_history[current_chat]:\n",
        "                #     if 'content' in message and isinstance(message['content'], list):\n",
        "                #         message['content'] = [item for item in message['content'] if item.get('type') != 'image_url']\n",
        "\n",
        "                try:\n",
        "                    history.append({\"role\": \"user\", \"content\": {\"path\": {file_path}}})\n",
        "\n",
        "                    # ì´ë¯¸ì§€ Base64 ë°ì´í„°ë¥¼ AIì¿¼ë¦¬ìš© historyì˜ contentì— ê¸°ë¡\n",
        "                    with open(file_path, \"rb\") as image_file:\n",
        "                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "                        image_base64 = f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                        ai_content.append({\"type\" : \"image_url\", \"image_url\": {\"url\" : image_base64}})\n",
        "\n",
        "                except Exception as e:\n",
        "                    history.append({\"role\": \"assistant\", \"content\": f\"Error processing file {file_path}: {str(e)}\"})\n",
        "\n",
        "            elif file_path.lower().endswith('.pdf'):\n",
        "                # PDF íŒŒì¼ ì²˜ë¦¬\n",
        "                pdf_text = extract_pdf_text(file_path)      #ì´ ë¶€ë¶„!!\n",
        "                if \"Error\" in pdf_text:\n",
        "                    history.append({\"role\": \"assistant\", \"content\": pdf_text})\n",
        "                else:\n",
        "                    #íŒŒì¼ í”„ë¡œì„¸ìŠ¤\n",
        "                    with open(file_path, 'rb') as file:\n",
        "                        file_name, text_chunks, metadata_chunks = process_file(file)\n",
        "                    # file_name, text_chunks, metadata_chunks = process_file(file_path)\n",
        "                    chunk_data.extend(text_chunks)\n",
        "                    chunk_metadata.extend(metadata_chunks)\n",
        "\n",
        "                    embeddings = embedding_model.encode(text_chunks, batch_size=32, show_progress_bar=True)\n",
        "                    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "                    d = embeddings.shape[1]\n",
        "\n",
        "                    if text_faiss_index is None:\n",
        "                        if(GPU_FLAG):\n",
        "                            # GPUìš© ì¸ë±ìŠ¤ ìƒì„±\n",
        "                            flat_config = faiss.GpuIndexFlatConfig()\n",
        "                            flat_config.device = 0  # ì‚¬ìš©í•  GPU ìž¥ì¹˜ ID ì„¤ì • (ë³´í†µ 0ë²ˆë¶€í„° ì‹œìž‘)\n",
        "                            text_faiss_index = faiss.GpuIndexFlatL2(res, d, flat_config)\n",
        "                        else:\n",
        "\n",
        "                            text_faiss_index = faiss.IndexFlatL2(d)\n",
        "                    text_faiss_index.add(embeddings)\n",
        "\n",
        "                    print(f\"{file_name} processed successfully. {len(text_chunks)} chunks indexed.\")\n",
        "\n",
        "                    ##ì¿¼ë¦¬ë¡œë¶€í„° ì„œì¹˜, ê²°ê³¼ + ì¿¼ë¦¬ hisotryì— ì¶”ê°€\n",
        "                    file_search = []\n",
        "                    if (message[\"text\"] != None):\n",
        "                        topk = search_file(message[\"text\"])\n",
        "                    for index, result in enumerate(topk):\n",
        "                        #history.append({\"role\": \"user\", \"content\": result})\n",
        "                        file_search.append(result)\n",
        "                    ai_content.append({\"type\" : \"text\", \"text\" : f\"Based on the following three sentences: 1. {file_search[0]}, 2. {file_search[1]}, 3. {file_search[2]}\"})\n",
        "                    print(\"search done!\")\n",
        "                    #history.append({\"role\": \"user\", \"content\": f\"PDF uploaded: {file_path}\\nExtracted text: {pdf_text[:500]}...\"}) #ì´ ë¶€ë¶„!!!\n",
        "            else:\n",
        "                # ì´ë¯¸ì§€ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°\n",
        "                history.append({\"role\": \"user\", \"content\": f\"File {file_path} is not supported.\"})\n",
        "\n",
        "    if (message[\"text\"]) == None:\n",
        "        history.append({\"role\": \"user\", \"content\": \"Please input any message\"})\n",
        "        return\n",
        "    elif (message[\"text\"]):\n",
        "        history.append({\"role\": \"user\", \"content\": message[\"text\"]})\n",
        "        ai_content.append({\"type\" : \"text\", \"text\" : message[\"text\"]})\n",
        "\n",
        "    print(ai_content)\n",
        "    # ì—…ë°ì´íŠ¸ëœ ížˆìŠ¤í† ë¦¬ë¥¼ ì €ìž¥\n",
        "    chat_histories[current_chat] = history\n",
        "    ai_history[current_chat] = [{\"role\": \"user\", \"content\" : ai_content}]\n",
        "    return history, gr.MultimodalTextbox(value=None, interactive=False)\n",
        "\n",
        "def bot(history: list):\n",
        "    global current_chat, chat_histories, ai_history\n",
        "\n",
        "    try:\n",
        "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "        # for msg in history:\n",
        "        #     if msg.get(\"image\"):\n",
        "        #         messages.append({\"role\": \"user\", \"content\": f\"Image data: {msg['image']}\"})\n",
        "        #     else:\n",
        "        #         messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "            messages=ai_history[current_chat],\n",
        "            stream=True\n",
        "        )\n",
        "\n",
        "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                history[-1][\"content\"] += chunk.choices[0].delta.content\n",
        "                time.sleep(0.05)\n",
        "                yield history\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append({\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"})\n",
        "        yield history\n",
        "\n",
        "    chat_histories[current_chat] = history\n",
        "\n",
        "def switch_chat(chat_id):\n",
        "    global current_chat, chat_histories\n",
        "    current_chat = chat_id\n",
        "    return chat_histories.get(chat_id, [])\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            new_chat_button = gr.Button(\"New Chat\")\n",
        "            chat_list = gr.Dropdown(\n",
        "                label=\"Chat Rooms\",\n",
        "                choices=[room[\"id\"] for room in chat_rooms],\n",
        "                value=\"Chat1\",\n",
        "            )\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(type=\"messages\")\n",
        "            chat_input = gr.MultimodalTextbox(\n",
        "                interactive=True,\n",
        "                file_count=\"multiple\",\n",
        "                placeholder=\"Enter message or upload file...\",\n",
        "                show_label=False,\n",
        "            )\n",
        "\n",
        "    chat_msg = chat_input.submit(\n",
        "        add_message, [chatbot, chat_input], [chatbot, chat_input]\n",
        "    )\n",
        "    bot_msg = chat_msg.then(bot, chatbot, chatbot, api_name=\"bot_response\")\n",
        "    bot_msg.then(lambda: gr.MultimodalTextbox(interactive=True), None, [chat_input])\n",
        "\n",
        "    new_chat_button.click(create_new_chat, [], [chat_list, chatbot])\n",
        "    chat_list.change(switch_chat, inputs=[chat_list], outputs=[chatbot])\n",
        "\n",
        "#gr.mount_gradio_app(app, demo, path=\"/gradio\")\n",
        "demo.launch(debug=True).share=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "664d7abc1cca415e9589016331fa5541",
            "fec0da8aed894284bba8c0552ab89ceb",
            "bd3e5866a2c9499e8adf4f8f8c975072",
            "326ab3e765a143cc957e9ba8d301289d",
            "8180bd1c04e44bcf8c20e7b5f5934200",
            "72018d5a3312488684b2755712b9fa4b",
            "ed7332ea28dc4f6cb9f1d5177c98f2b6",
            "d2ab2e55a0c34dc186f5ea0021acd4c6",
            "c07c20fc84474a41939f3b8a6d11ddfa",
            "a13ddd001e7c49f983fd97cea21d38d8",
            "5a569fdc62f7467e9fb57c02a9578b35"
          ]
        },
        "id": "4qldkGWiXOf7",
        "outputId": "88d23e6c-956d-41ba-b043-64efd4765ece"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f0b8994821588c8a18.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0b8994821588c8a18.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/gradio/587909888c9923e3fa0dc8f6818d9ca5566f2bf63568dd6f94b54b6dd2d9704a/Lecture-01-LLM-Input-Output.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664d7abc1cca415e9589016331fa5541"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lecture-01-LLM-Input-Output.pdf processed successfully. 68 chunks indexed.\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab sizeë¡œ ë¶€í„° encodeëœ token idë¥¼ vectorë¡œ 1ëŒ€1 mapping vector setëŠ” vocab size ë§Œí¼ì˜ vector tableë¡œ êµ¬ì„± embedded vectorëŠ” learnable paramet...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab sizeë¡œ ë¶€í„° encodeëœ token idë¥¼ vectorë¡œ 1ëŒ€1 mapping vector setëŠ” vocab size ë§Œí¼ì˜ vector tableë¡œ êµ¬ì„± embedded vectorëŠ” learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: ì¢Œí‘œì‹œìŠ¤í…œ ëŒ€ì‹ ì—, ìž„ì˜ì˜ vector spaceë¥¼ W matrixë¡œ í‘œí˜„í•˜ë©´, vector xë¥¼ Wì— projectioní•˜ëŠ” ì‹ì€, ð‘Šð‘¥ð‘‡ ê°€ ëœë‹¤. LLMì—ì„œëŠ” token embeddingì— ì‚¬ìš©ëœ nn.Embedding weightsë¥¼ ìž¬ì‚¬ìš© weight sharing Weights matrixë¥¼ Wë¼ í•˜ê³ , output sequence ë¥¼ matri...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab sizeë¡œ ë¶€í„° encodeëœ token idë¥¼ vectorë¡œ 1ëŒ€1 mapping vector setëŠ” vocab size ë§Œí¼ì˜ vector tableë¡œ êµ¬ì„± embedded vectorëŠ” learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: ì¢Œí‘œì‹œìŠ¤í…œ ëŒ€ì‹ ì—, ìž„ì˜ì˜ vector spaceë¥¼ W matrixë¡œ í‘œí˜„í•˜ë©´, vector xë¥¼ Wì— projectioní•˜ëŠ” ì‹ì€, ð‘Šð‘¥ð‘‡ ê°€ ëœë‹¤. LLMì—ì„œëŠ” token embeddingì— ì‚¬ìš©ëœ nn.Embedding weightsë¥¼ ìž¬ì‚¬ìš© weight sharing Weights matrixë¥¼ Wë¼ í•˜ê³ , output sequence ë¥¼ matri...\n",
            "Result 4: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 16 | Distance: 1.4060\n",
            "Content: Position Embedding in LLM Why do we need positional embedding? Transformer Networkì—ëŠ” sequence orderê°œë…ì„ ì²˜ë¦¬í•  ëŠ¥ë ¥ì´ ì—†ë‹¤. â‘  matrixmultiplication â‘¡ layer normalization â‘¢ MLP multilayer perceptron â‘£ attention ...\n",
            "Result 1: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 3 | Distance: 1.1657\n",
            "Content: LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme...\n",
            "Result 2: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 14 | Distance: 1.3843\n",
            "Content: Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab sizeë¡œ ë¶€í„° encodeëœ token idë¥¼ vectorë¡œ 1ëŒ€1 mapping vector setëŠ” vocab size ë§Œí¼ì˜ vector tableë¡œ êµ¬ì„± embedded vectorëŠ” learnable paramet...\n",
            "Result 3: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 28 | Distance: 1.4001\n",
            "Content: ì¢Œí‘œì‹œìŠ¤í…œ ëŒ€ì‹ ì—, ìž„ì˜ì˜ vector spaceë¥¼ W matrixë¡œ í‘œí˜„í•˜ë©´, vector xë¥¼ Wì— projectioní•˜ëŠ” ì‹ì€, ð‘Šð‘¥ð‘‡ ê°€ ëœë‹¤. LLMì—ì„œëŠ” token embeddingì— ì‚¬ìš©ëœ nn.Embedding weightsë¥¼ ìž¬ì‚¬ìš© weight sharing Weights matrixë¥¼ Wë¼ í•˜ê³ , output sequence ë¥¼ matri...\n",
            "Result 4: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 16 | Distance: 1.4060\n",
            "Content: Position Embedding in LLM Why do we need positional embedding? Transformer Networkì—ëŠ” sequence orderê°œë…ì„ ì²˜ë¦¬í•  ëŠ¥ë ¥ì´ ì—†ë‹¤. â‘  matrixmultiplication â‘¡ layer normalization â‘¢ MLP multilayer perceptron â‘£ attention ...\n",
            "Result 5: File: Lecture-01-LLM-Input-Output.pdf | Type: PDF | Page: 23 | Distance: 1.4738\n",
            "Content: Input Output Training Example for LM This is a sample chunk of words. This is a sample chunk of words. The transformer model should The transformer model should predict next predict next word embeddin...\n",
            "search done!\n",
            "[{'type': 'text', 'text': 'Based on the following three sentences: 1. LLM Process embedding deembedding each tokens transformer each vectors texts token converted into decoder token texts converted into a learnable blocks a token vector tokenizing, embedding, transforme..., 2. Embedding in LLM 1. Token Embedding From tokens to vectors tokenizer vocab sizeë¡œ ë¶€í„° encodeëœ token idë¥¼ vectorë¡œ 1ëŒ€1 mapping vector setëŠ” vocab size ë§Œí¼ì˜ vector tableë¡œ êµ¬ì„± embedded vectorëŠ” learnable paramet..., 3. ì¢Œí‘œì‹œìŠ¤í…œ ëŒ€ì‹ ì—, ìž„ì˜ì˜ vector spaceë¥¼ W matrixë¡œ í‘œí˜„í•˜ë©´, vector xë¥¼ Wì— projectioní•˜ëŠ” ì‹ì€, ð‘Šð‘¥ð‘‡ ê°€ ëœë‹¤. LLMì—ì„œëŠ” token embeddingì— ì‚¬ìš©ëœ nn.Embedding weightsë¥¼ ìž¬ì‚¬ìš© weight sharing Weights matrixë¥¼ Wë¼ í•˜ê³ , output sequence ë¥¼ matri...'}, {'type': 'text', 'text': 'tell me about llm'}]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f0b8994821588c8a18.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scVGd5Za2Iwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}